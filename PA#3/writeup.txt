I began the assignment by trying to understand what the worker role and web role should be doing and developed a gameplan to give me direction in how to start.  I began by creating the webrole and the worker role.  For the webrole, I wrote two main methods: Start and Stop.  The start would begin the crawling action but adding the two robots.txt links to an urlQueue and add a start command for the crawler.  The worker role is constantly running.  When the start command is read from the command queue but the worker role, it enters into the crawling code.  It stays in this crawling code while a stop command is not read from the command queue.  The logic took some time to get right because it needed to work when there was only one command message (the initial start) and when it has been started, stopped, and started again.

The crawling code of the while loop takes the next url in the urlQueue, starting with the two robots.txt files.  It then checks whether it is a robots.txt file, XML file, or a normal webpage.  If it is a robots.txt page, it opens the page contents and uses a stream reader.  Since the entire page is a list, I split each line by a single space to divide the label (Sitemap: and Disallows) from the XML uri.  Going by each line, if the first part is Sitemap, I add the second part of the line into the urlQueue.  If it is Disallow, I concatenate the relative path with the domain which I got by removing the /robots.txt from the original url, and add it to a Disallow hashset.  Originally, I used Lists to keep track of disallows and checked urls but switched to hashset because it is supposed to make checking faster.  If the url is an XML file, I view the contents of the page using XMLDocument and add all links in Bleachreport and all links from CNN that follow the 2015 year format.  They used different namespaces so I needed to distinguish the two.  If the url is a normal website, I used htmlAgility to get all the href links on the page, checked if they followed the format we wanted, if they have already been visited, then add them to the urlQueue.  They are also added to the TableStorage after being encoded and checked if the url is already there.  The crawler moves onto the next url in the url queue if a stop command hasn’t been sent.

I created the html page and used javascript and ajax to make the functionality of the dashboard.  As I added things to the dashboard, I created more webmethods in the webrole that did the various functions.  I used buttons to trigger the start, stop, and delete index tables methods.  I included the various stats within a document onReady function that updates when the page is refreshed.
